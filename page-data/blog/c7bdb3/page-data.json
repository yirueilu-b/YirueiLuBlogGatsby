{"componentChunkName":"component---src-templates-post-js","path":"/blog/c7bdb3","result":{"pageContext":{"md_path":"mediapipeHandTracking","title":"Multiple Hand Tracking Using MediaPipe Models in Python","description":"Google MediaPipe has released their paper and models of hand tracking recently. After reading the paper, I am so impressed by their work. They use som","image":"https://i.imgur.com/x5xVgad.png","uuid":"c7bdb3","text":"# Multiple Hand Tracking Using MediaPipe Models in Python\r\n\r\nGoogle MediaPipe has released their paper and models of hand tracking recently. After reading the paper, I am so impressed by their work. They use some clever methods to simplify the hand tracking task thus make it possible to do this task in real-time and keep great performance. Then I start to try playing with the models in Python, In this post, I take some notes of how to use their models in Python and create a simple demo with webcam. The complete code could be found [here](https://github.com/yirueilu-b/HandTracking)\r\n\r\n## The Hand Tracking Workflow\r\n\r\nAs the graph provided by MediaPipe below, we could summarize the steps which multiple hand tracking process takes like: \r\n\r\n$numHand = 0$\r\n\r\n$isFirstFrame = True$\r\n\r\n$while \\space True:$\r\n\r\n$\\quad frame = readFrame()$\r\n\r\n$\\quad inputFrame = preprocessImage(frame)$\r\n\r\n$\\quad if \\space isFirstFrame \\space or \\space numHand < expectNumHand:$\r\n\r\n$\\quad \\quad isFirstFrame = False$\r\n\r\n$\\quad \\quad keyPoints = detectPalm(inputFrame)$\r\n\r\n$\\quad \\quad hand = cropHandFromPalmRes(inputFrame, \r\nkeyPoints)$\r\n\r\n$\\quad else:$\r\n\r\n$\\quad \\quad hand = cropHandFromPrevRes(inputFrame, \r\nlandmark)$\r\n\r\n$\\quad landmark, isHand = detectLandmark(hand)$\r\n\r\n$\\quad drawLandmark()$\r\n\r\n![](https://i.imgur.com/x5xVgad.png)\r\n\r\n## Write the Code of Components in Workflow\r\n\r\nAccording to the above algorithm, we could write the actual Python code to implement the hand tracking demo now\r\n\r\n### preprocessImage\r\n\r\nThe input image of palm detector should be: \r\n- converted to `RGB`\r\n- padded to square with zeros\r\n- resized to `(256, 256)`\r\n- normalized by `2 * ((image / 255) - 0.5)`\r\n\r\n```python\r\ndef preprocess(bgr_image, w, h):\r\n    # convert to rgb\r\n    rgb_image = bgr_image[:, :, ::-1]\r\n    # pad to square and resize\r\n    shape = np.r_[rgb_image.shape]\r\n    padding = (shape.max() - shape[:2]).astype('uint32') // 2\r\n    rgb_image = np.pad(rgb_image, ((padding[0], padding[0]), (padding[1], padding[1]), (0, 0)), mode='constant')\r\n    padding_image = rgb_image.copy()\r\n\r\n    rgb_image = cv2.resize(rgb_image, (w, h))\r\n    # normalize\r\n    input_image = np.ascontiguousarray(2 * ((rgb_image / 255) - 0.5).astype('float32'))\r\n    padding_image = np.ascontiguousarray(2 * ((padding_image / 255) - 0.5).astype('float32'))\r\n    return input_image, padding_image, padding\r\n```\r\n\r\n![](https://i.imgur.com/rbBgVNT.png)\r\n\r\n### detectPalm\r\n\r\nPass the preprocessed image to `detectPalm` to get corresponding 7 key points and bounding box of palm\r\n\r\n![](https://i.imgur.com/n4pGw1I.png)\r\n\r\n```python\r\n# inference\r\ndef detect_palm(input_image, palm_detector, input_details, output_details):\r\n    palm_detector.set_tensor(input_details[0]['index'], input_image.reshape(1, 256, 256, 3))\r\n    palm_detector.invoke()\r\n    output_reg = palm_detector.get_tensor(output_details[0]['index'])[0]\r\n    output_clf = palm_detector.get_tensor(output_details[1]['index'])[0, :, 0]\r\n    return output_reg, output_clf\r\n```\r\n\r\n>shape of output_reg -> (number of anchors, number of predictions)\r\n>shape of output_clf -> (number of anchors, 1)\r\n>- number of anchors = 2944\r\n>- number of predictions = 18 \r\n>    - 0 to 4 are bounding box offset, width and height: dx, dy, w ,h\r\n>    - 4 to 18 are 7 hand keypoint x and y coordinates: x1, y1, x2, y2, ..., x7, y7\r\n>\r\n>**output_reg: bounding boxes and keypoints**\r\n>**output_clf: confidence of each anchors**\r\n\r\n```python\r\n# get actual coordinate of key points from prediction\r\ndef get_res_from_palm_detector(output_reg, output_clf):\r\n    # normalize scores to range 0 to 1 using sigmoid\r\n    scores = sigmoid(output_clf)\r\n    # filter by threshold\r\n    output_reg = output_reg[scores > DETECTION_THRESHOLD]\r\n    output_clf = output_clf[scores > DETECTION_THRESHOLD]\r\n    candidate_anchors = anchors[scores > DETECTION_THRESHOLD]\r\n    if output_reg.shape[0] == 0: print(\"No hands found\")\r\n    # get actual coordinate by pre-defined anchor\r\n    moved_output_reg = output_reg.copy()\r\n    moved_output_reg[:, :2] = moved_output_reg[:, :2] + candidate_anchors[:, :2] * 256\r\n    # NMS for bounding boxes\r\n    box_ids = fast_nms(moved_output_reg[:, :4], output_clf, NMS_THRESHOLD)\r\n    # convert the coordinates back to the scale in original image size\r\n    box_list = moved_output_reg[box_ids, :4].astype('int')\r\n    side_list = []\r\n    key_point_list = moved_output_reg[box_ids, 4:].reshape(-1, 7, 2)\r\n    center_wo_offst = candidate_anchors[box_ids, :2] * 256\r\n    for i in range(len(key_point_list)):\r\n        key_point_list[i] = key_point_list[i] + center_wo_offst[i]\r\n        x, y, w, h = box_list[i]\r\n        side_list.append(max(w, h) * BOX_ENLARGE)\r\n    return key_point_list, side_list\r\n```\r\n\r\n![](https://i.imgur.com/ftZrqPW.png)\r\n\r\n### preprocessForLandmarkModel\r\n\r\nUse the keypoints to crop hand on image and rotate to the specific angle. With `getAffineTransform` function, we could define the matrices then get the cropped hand image directly.\r\n\r\n![](https://i.imgur.com/fYFInLE.png)\r\n\r\n```python\r\ndef get_hand(input_image, key_points, side):\r\n    source = get_triangle(key_points[0], key_points[2], side)\r\n    source -= (key_points[0] - key_points[2]) * BOX_SHIFT\r\n    transform_mat = cv2.getAffineTransform(source * max(input_image.shape) / INPUT_WIDTH, TARGET_TRIANGLE)\r\n    hand = cv2.warpAffine(input_image, transform_mat, (INPUT_WIDTH, INPUT_HEIGHT))\r\n    return hand, source\r\n```\r\n\r\n### detectLandmark\r\n\r\nPass the preprocessed image to `detectLandmark` to get corresponding 7 key points and bounding box of hand\r\n\r\n![](https://i.imgur.com/AsUnjLW.png)\r\n\r\n```python\r\ndef detect_landmark(hand, landmark_model, input_details, output_details):\r\n    landmark_model.set_tensor(input_details[0]['index'], hand.reshape(1, 256, 256, 3))\r\n    landmark_model.invoke()\r\n    landmark = landmark_model.get_tensor(output_details[0]['index']).reshape(-1, 2)\r\n    is_hand = landmark_model.get_tensor(output_details[1]['index']) > HAND_THRESHOLD\r\n    return landmark, is_hand\r\n```\r\n```python\r\ndef convert_landmark_back(joints, source, padding, image):\r\n    # projecting keypoints back into original image coordinate space\r\n    transform_mat = cv2.getAffineTransform(source * max(image.shape) / INPUT_WIDTH, TARGET_TRIANGLE)\r\n    transform_mat = np.pad(transform_mat.T, ((0, 0), (0, 1)), constant_values=1, mode='constant').T\r\n    transform_mat[2, :2] = 0\r\n    transform_mat_inv = np.linalg.inv(transform_mat)\r\n    landmark = (np.pad(joints, ((0, 0), (0, 1)), constant_values=1, mode='constant') @ transform_mat_inv.T)[:, :2]\r\n    landmark -= padding[::-1]\r\n\r\n    # projecting keypoints back into input image coordinate space\r\n    landmark_input = landmark + padding[::-1]\r\n    landmark_input = landmark_input * INPUT_WIDTH / max(image.shape)\r\n    return landmark, landmark_input\r\n```\r\n\r\n![](https://i.imgur.com/DeNgmEH.png)\r\n\r\n### Visualize the Hand Tracking Process\r\n\r\n![](https://i.imgur.com/oxYN0kv.png)\r\n\r\n## Pulling it all together\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom hand_tracking_utils import *\r\n\r\nWINDOW_NAME = 'MediaPipe Hand Tracking'\r\nPALM_MODEL_PATH = os.path.join('models', 'palm_detection_without_custom_op.tflite')\r\nLANDMARK_MODEL_PATH = os.path.join('models', 'hand_landmark.tflite')\r\nNUM_TRACK_HAND = 2\r\n\r\nif __name__ == '__main__':\r\n    cap = cv2.VideoCapture(1)\r\n    cv2.resizeWindow(WINDOW_NAME, IMAGE_WIDTH, IMAGE_HEIGHT)\r\n    # load palm model\r\n    palm_model = tf.lite.Interpreter(model_path=PALM_MODEL_PATH)\r\n    palm_model.allocate_tensors()\r\n    palm_input_details = palm_model.get_input_details()\r\n    palm_output_details = palm_model.get_output_details()\r\n    # load landmark model\r\n    landmark_model = tf.lite.Interpreter(model_path=LANDMARK_MODEL_PATH)\r\n    landmark_model.allocate_tensors()\r\n    landmark_input_details = landmark_model.get_input_details()\r\n    landmark_output_details = landmark_model.get_output_details()\r\n    # out = cv2.VideoWriter('output.mp4', -1, 5., (640, 480))\r\n    num_valid_hand = 0\r\n    is_first_frame = True\r\n    prev_res = None\r\n    while True:\r\n        # read and preprocess a frame\r\n        _, frame = cap.read()\r\n        input_image, padding_image, padding = preprocess(frame, INPUT_WIDTH, INPUT_HEIGHT)\r\n        if is_first_frame or num_valid_hand < NUM_TRACK_HAND:\r\n            print(\"Palm Detector Activated!\")\r\n            is_first_frame = False\r\n            output_reg, output_clf = detect_palm(input_image, palm_model, palm_input_details, palm_output_details)\r\n            key_point_list, side_list = get_res_from_palm_detector(output_reg, output_clf)\r\n        else:\r\n            print(\"Palm Detector Not Activated!\")\r\n            key_point_list, side_list = get_res_from_prev_res(prev_res)\r\n\r\n        prev_res = []\r\n        for i in range(len(key_point_list)):\r\n            hand, source = get_hand(padding_image, key_point_list[i], side_list[i])\r\n            landmark, is_hand = detect_landmark(hand, landmark_model, landmark_input_details, landmark_output_details)\r\n            if is_hand:\r\n                landmark, landmark_input = convert_landmark_back(landmark, source, padding, frame)\r\n                frame = draw_landmark(frame, landmark)\r\n                prev_res.append(landmark_input)\r\n\r\n        num_valid_hand = len(prev_res)\r\n        cv2.imshow(WINDOW_NAME, frame)\r\n        # out.write(original_frame)\r\n\r\n        # press 'q' to exit\r\n        if cv2.waitKey(1) & 0xFF == ord('q'):\r\n            cap.release()\r\n            cv2.destroyAllWindows()\r\n            break\r\n\r\n    cap.release()\r\n    cv2.destroyAllWindows()\r\n```\r\n\r\n## Result\r\n\r\n![](https://i.imgur.com/dRv6k7w.gif)\r\n\r\n\r\n###### tags: `Machine Learning`"}},"staticQueryHashes":[]}